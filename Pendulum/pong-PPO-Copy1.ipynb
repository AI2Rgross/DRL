{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.70963291,  0.70457159, -0.29201214],\n",
       "       [-0.88121757,  0.47271091, -0.30510363],\n",
       "       [-0.24428608,  0.96970321, -0.41104684],\n",
       "       [-0.33505524, -0.94219849, -0.81791723]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n"
     ]
    }
   ],
   "source": [
    "nb_agent=4\n",
    "envs = parallelEnv('Pendulum-v0', n=nb_agent)\n",
    "envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units,fc11_units, std=0.0):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1  = nn.Linear(state_size, fc1_units)\n",
    "        self.fc11 = nn.Linear(fc1_units, fc11_units)\n",
    "        self.fc2v = nn.Linear(fc11_units, 1)\n",
    "        self.fc2a = nn.Linear(fc11_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size) * std)   \n",
    "   \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.normal_(mean=0., std=0.1)\n",
    "        self.fc11.weight.data.normal_(mean=0., std=0.1)\n",
    "        self.fc2a.weight.data.normal_(mean=0., std=0.1)\n",
    "        self.fc2v.weight.data.normal_(mean=0., std=0.1)\n",
    "       \n",
    "    def forward(self, state):\n",
    " \n",
    "        value = F.relu(self.fc1(state))\n",
    "        value = F.relu(self.fc11(value))\n",
    "        value = self.fc2v(value)\n",
    "\n",
    "        mu = F.relu(self.fc1(state))\n",
    "        mu = F.relu(self.fc11(mu))\n",
    "        mu = self.fc2a(mu)\n",
    "\n",
    "\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        distribution  = Normal(mu, std)\n",
    "        return distribution, value\n",
    "    \n",
    " \n",
    "# run your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=Policy(state_size=3, action_size=1,fc1_units=128,fc11_units=64).to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "        \n",
    "    n=len(envs.ps)\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list =[]\n",
    "    reward_list=[]\n",
    "    prob_list  =[]\n",
    "    action_list=[]\n",
    "    value_list =[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    #start all parallel agents\n",
    "\n",
    "#    envs.step([[1]]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for i in range(nrand):\n",
    "        action = [[np.random.uniform(-2, 2)] for _ in range(n)]\n",
    "        next_states, rewards, dones, _ = envs.step(action)\n",
    "\n",
    "    log_probs=[]\n",
    "    \n",
    "    for t in range(tmax):\n",
    "        actions=[]\n",
    "        state_list.append(next_states)\n",
    "        values=[]\n",
    "        for state in next_states:\n",
    " \n",
    "            state=torch.tensor([state], dtype=torch.float, device=device)\n",
    "            distribution, value = policy(state)\n",
    "            action = distribution.sample()\n",
    "            log_prob = distribution.log_prob(action).detach()\n",
    " #           entropy  = distribution.entropy()\n",
    "            actions.append(action.squeeze())\n",
    "            values.append(value.squeeze().data)\n",
    "            log_probs.append(log_prob.squeeze()) \n",
    " #            entropies.append(entropy.squeeze())\n",
    "        next_states, rewards, is_dones,_= envs.step(actions)\n",
    "\n",
    "        # store the result\n",
    " #       print(\"rewards:\",rewards)\n",
    " #       print(\"values:\",values)\n",
    " #       print(\"rewards_values:\",rewards-values)\n",
    "\n",
    "        reward_list.append(rewards)\n",
    "        action_list.append(actions)\n",
    "        value_list.append(actions)        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_dones.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return log_probs, state_list,action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c53187aa74c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mold_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnrand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"actions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"probs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "old_probs, states, actions, rewards, values = collect_trajectories(envs, policy, tmax=4,nrand=2)\n",
    "print(\"actions\",actions)\n",
    "print(\"probs\",old_probs)\n",
    "print(\"values\",values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clipped surrogate\n",
    "In PPO algorithm the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "def clipped_surrogate(policy, old_log_probs, states, actions, rewards,values,discount=0.995,epsilon=0.1, beta=0.01):\n",
    "    actions=[]\n",
    "    new_log_probs=[]\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]-values\n",
    "  #  print(\"rewards F:\",rewards)\n",
    " #   print(\"values \",values)\n",
    "    # normalize the reward: (x-mean)/std \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    # convert states to policy (or probability)\n",
    "    # evaluate the new prob: this is what we have:(s,a,r,s') for old prob. \n",
    "    new_log_probs=[]\n",
    " \n",
    "    for state in states:\n",
    "        for s in state:\n",
    "            s=torch.tensor([s], dtype=torch.float, device=device)\n",
    "            distribution, value = policy(s)   \n",
    "            action = distribution.sample()\n",
    "            log_prob = distribution.log_prob(action)\n",
    "            new_log_probs.append(log_prob.squeeze())\n",
    "\n",
    "    new_log_probs=torch.stack(new_log_probs)\n",
    "    old_log_probs=torch.stack(old_log_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio=(new_log_probs-old_log_probs).exp()\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio,min= 1-epsilon,max= 1+epsilon)\n",
    "    rewards=rewards.view(1,-1).squeeze()\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "    entropy = -(new_log_probs.exp()*torch.log(old_log_probs.exp()+1.e-10)+ \\\n",
    "        (1.0-new_log_probs.exp())*torch.log(1.0-old_log_probs.exp()+1.e-10))\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_probs, states, actions, rewards,values = collect_trajectories(envs, policy, tmax=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: pip: not found\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
      "/home/raphe/.local/lib/python3.5/site-packages/gym/envs/classic_control/pendulum.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  u = np.clip(u, -self.max_torque, self.max_torque)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards F: [[ -3.75804079  -3.97351025  -7.23574814  -0.11624235]\n",
      " [ -4.92076418  -5.11010118  -8.11144059  -0.14097114]\n",
      " [ -6.68368676  -6.5909249   -9.14566223  -0.1816862 ]\n",
      " [ -8.67663005  -8.52035806 -10.32038886  -0.21980922]\n",
      " [-10.94420271 -10.49861404 -10.29599292  -0.37125198]\n",
      " [-12.9833666  -12.56264345  -9.28055054  -0.65921392]\n",
      " [-13.38678613 -12.00565198  -8.22751792  -0.93328958]\n",
      " [-10.96269897 -10.1828688   -7.19661239  -1.24173368]\n",
      " [ -8.86831915  -8.51365152  -6.32883441  -1.58341299]\n",
      " [ -6.95468268  -6.71823082  -5.61744979  -2.42042804]\n",
      " [ -5.31555995  -5.22873563  -5.00551909  -3.14466467]\n",
      " [ -4.03368941  -4.10770787  -4.64158874  -4.1877801 ]\n",
      " [ -3.0668535   -3.12393453  -4.44884767  -5.57785857]\n",
      " [ -2.27463859  -2.32897973  -4.49262009  -7.09503327]\n",
      " [ -1.74724868  -1.91805086  -4.78751228  -9.17950146]]\n",
      "values  [[tensor(-0.2342), tensor(-0.1999), tensor(-0.1758), tensor(-0.1711)], [tensor(-0.2635), tensor(-0.2115), tensor(-0.1779), tensor(-0.1724)], [tensor(-0.3132), tensor(-0.2357), tensor(-0.1757), tensor(-0.1731)], [tensor(-0.3630), tensor(-0.2745), tensor(-0.1876), tensor(-0.1742)], [tensor(-0.4102), tensor(-0.3001), tensor(-0.1868), tensor(-0.1702)], [tensor(-0.4308), tensor(-0.3159), tensor(-0.1851), tensor(-0.1671)], [tensor(-0.4645), tensor(-0.3111), tensor(-0.1718), tensor(-0.1705)], [tensor(-0.4322), tensor(-0.2930), tensor(-0.1502), tensor(-0.1772)], [tensor(-0.3920), tensor(-0.2697), tensor(-0.1249), tensor(-0.1844)], [tensor(-0.3445), tensor(-0.2310), tensor(-0.1076), tensor(-0.2197)], [tensor(-0.2998), tensor(-0.1865), tensor(-0.1149), tensor(-0.2410)], [tensor(-0.2482), tensor(-0.1444), tensor(-0.1435), tensor(-0.2711)], [tensor(-0.2010), tensor(-0.0955), tensor(-0.1850), tensor(-0.3089)], [tensor(-0.1567), tensor(-0.0586), tensor(-0.2168), tensor(-0.3432)], [tensor(-0.1391), tensor(-0.0841), tensor(-0.2219), tensor(-0.3942)]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (60) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-fdc1a177f0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclipped_surrogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-5b139707689c>\u001b[0m in \u001b[0;36mclipped_surrogate\u001b[0;34m(policy, old_log_probs, states, actions, rewards, values, discount, epsilon, beta)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# ratio for clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_log_probs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mold_log_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# clipped function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (60) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 1\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('Pendulum-v0', n=nb_agent, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .00001\n",
    "tmax = 15\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards,values = collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        L = -clipped_surrogate(policy, old_probs, states, actions, rewards,values,epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%5==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "torch.save(policy, 'PPO.policy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, time=2000, nrand=5,n=1):\n",
    "\n",
    "    env.reset()\n",
    "    # perform nrand random steps in the beginning\n",
    "\n",
    "    for i in range(nrand):\n",
    "        next_state, reward, done, _ = env.step([np.random.uniform(-2, 2)])\n",
    "    \n",
    "    for _ in range(time):\n",
    "        next_state=torch.tensor([next_state], dtype=torch.float, device=device)   \n",
    "        distribution, value = policy(next_state)\n",
    "        action = distribution.sample()\n",
    "        env.render()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "    \n",
    "    env.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "policy = torch.load('PPO.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_solution = torch.load('PPO_solution.policy')\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, policy, time=2000, nrand=5,n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
